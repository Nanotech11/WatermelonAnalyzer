{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f26ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration\n",
    "## If you are training a new model, set is_training to True, or False to load a pretrained model from a pkl file\n",
    "is_training = False\n",
    "\n",
    "## If training a new model, this is the file name that the model will be saved to, if not training this will be the name of the pkl file to load.\n",
    "file_name = '512x512epochs.pkl'\n",
    "\n",
    "## Set mae_loss to True to use Mean Absolute Error for the loss function, or False to use Mean Squared Error.\n",
    "mae_loss = True\n",
    "\n",
    "## Set use_standardization to True to use standardization for feature scaling, or False to use normalization.\n",
    "use_standardization = True\n",
    "\n",
    "## Batch size of 28 works for my 16GB of VRAM. However, this value will depend on your specs. Use smaller values if less VRAM is available.\n",
    "batch_size = 28\n",
    "\n",
    "## Set the size in pixels that the images should be scaled to. Higher counts will require more VRAM and take longer per epoch to train.\n",
    "## However, higher values may allow the model to converge with fewer total epochs and achieve a higher accuracy.\n",
    "img_size = 512\n",
    "\n",
    "## Set the learning rate to a custom value. Or, to use lr_find to automatically calculate a good learning rate, set lr = 0 (this is the recommended default).\n",
    "lr = 0\n",
    "\n",
    "## Set the number of epochs to train for. Should probably be >= ~200 to yield a usable model.\n",
    "n_epochs = 512\n",
    "\n",
    "## Set the number of epochs to train while the body of the weights is frozen. This should not be set too high to avoid overfitting (default = 1).\n",
    "n_freeze_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea20a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in y values.\n",
    "df = pd.read_csv('PhenotypeDataUGA.csv').drop('line', axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract column names.\n",
    "traits = df.columns.values.tolist()[1:]\n",
    "traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ddeae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Scaling\n",
    "scaler = StandardScaler() if use_standardization else MinMaxScaler()\n",
    "df.iloc[:, 1:] = pd.DataFrame(scaler.fit_transform(df.iloc[:, 1:]), columns=traits)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine values in each row to a list to pass into the model.\n",
    "df['combined'] = df[traits].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eab45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the dataset.\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitledList(list, ShowTitle):\n",
    "    _show_args = {'label': 'text'}\n",
    "    \n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show self\"\n",
    "        return show_title(self, ctx=ctx, **merge(self._show_args, kwargs))\n",
    "\n",
    "class ToListTensor(DisplayedTransform):\n",
    "    _show_args = {'label': 'text'}\n",
    "    \n",
    "    def __init__(self, split_idx=None,):\n",
    "        super().__init__(split_idx=split_idx)\n",
    "\n",
    "    def encodes(self, o): return o\n",
    "    \n",
    "    def decodes(self, o): return TitledList(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the datablock. Of particular importance are the resize dimensions, method, and pad mode.\n",
    "## The model gets better results faster when using high resolution images.\n",
    "## Padding with zeros ensures that there is no loss of aspect ratio or information compared to squishing or cropping.\n",
    "plant = DataBlock(blocks = [ImageBlock, RegressionBlock(n_out=12)],\n",
    "                  get_x = ColReader('photo_id', pref=f'fruits/IMG_', suff='.JPG'),\n",
    "                  get_y = Pipeline( [ColReader('combined'), ToListTensor ]),\n",
    "                  splitter = RandomSplitter(valid_pct=0.1),\n",
    "                  item_tfms = Resize(img_size, method=ResizeMethod.Pad, pad_mode=PadMode.Zeros),\n",
    "                  n_inp = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a1b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the dataloader.\n",
    "dls = plant.dataloaders(train, bs=batch_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the learner using resnet50 as the initial weights.\n",
    "learn = vision_learner(\n",
    "               dls = dls,\n",
    "               arch = resnet50,\n",
    "               metrics = [mae, mse, rmse, R2Score()],\n",
    "               loss_func = L1LossFlat() if mae_loss else MSELossFlat(),\n",
    "               n_out = 12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30970db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If training, set learning rate. When training, lr_find can be useful for picking a more optimal learning rate.\n",
    "if is_training and lr <= 0:\n",
    "    lrs = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n",
    "    lr = lrs.valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_training:\n",
    "    ## Train the model and export to file.\n",
    "    learn.fine_tune(epochs=n_epochs, base_lr=lr, freeze_epochs=n_freeze_epochs)\n",
    "    learn.export(file_name)\n",
    "else:\n",
    "    ## Load model for testing\n",
    "    learn = load_learner(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict on the test dataset.\n",
    "dl = learn.dls.test_dl(test)\n",
    "preds, _ = learn.get_preds(dl=dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00464bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract list of photo IDs.\n",
    "photo_id_col = test[\"photo_id\"].tolist()\n",
    "\n",
    "## Undo feature scaling.\n",
    "preds = pd.DataFrame(scaler.inverse_transform(preds), columns=traits)\n",
    "\n",
    "## Append the photo_id column\n",
    "preds['photo_id'] = photo_id_col\n",
    "preds.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop photo_id and combined as undoing feature scaling using sklearn returns a numpy array, losing the dataframe's column names.\n",
    "test = test.drop(['photo_id', 'combined'], axis=1, errors='ignore')\n",
    "test.index = range(len(test.index))\n",
    "\n",
    "## Undo feature scaling.\n",
    "test = pd.DataFrame(scaler.inverse_transform(test), columns=traits)\n",
    "\n",
    "## Append the photo_id column\n",
    "test['photo_id'] = photo_id_col\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create empty dataframe to combine test and preds into.\n",
    "test_preds_combined = pd.DataFrame()\n",
    "\n",
    "## Iterate through test and preds and append each row. Even rows are test data, the next index is the predicted value.\n",
    "for i in range(len(preds)):\n",
    "    test_preds_combined = pd.concat([test_preds_combined, test.iloc[i]], axis=1)\n",
    "    test_preds_combined = pd.concat([test_preds_combined, preds.iloc[i]], axis=1)\n",
    "\n",
    "## Transform dataframe to have the same structure as the originals.\n",
    "test_preds_combined = test_preds_combined.T.reset_index(drop=True)\n",
    "\n",
    "## Export dataframe to a csv file.\n",
    "test_preds_combined.to_csv(file_name + '_combined.csv')\n",
    "test_preds_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate regression metrics for all columns combined except photo_id.\n",
    "print('Mean squared error: ' + str(mean_squared_error(test.iloc[:, :-1], preds.iloc[:, :-1])))\n",
    "print('Mean absolute error: ' + str(mean_absolute_error(test.iloc[:, :-1], preds.iloc[:, :-1])))\n",
    "print('R2 score: ' + str(r2_score(test.iloc[:, :-1], preds.iloc[:, :-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73320643",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate regression metrics for each column individually.\n",
    "for i, name in enumerate(traits):\n",
    "    print('Mean squared error for column ' + name + ': ' + str(mean_squared_error(test.iloc[:, i], preds.iloc[:, i])))\n",
    "    print('Mean absolute error for column ' + name + ': ' + str(mean_absolute_error(test.iloc[:, i], preds.iloc[:, i])))\n",
    "    print('R2 score for column ' + name + ': ' + str(r2_score(test.iloc[:, i], preds.iloc[:, i])) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0674b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## valid_ranges stores margins of error to be checked.\n",
    "valid_ranges = dict.fromkeys([0.1, 0.05, 0.025, 0.01])\n",
    "## Iterate through each margin of error and save each row where every column is within that percent of the test values. \n",
    "for percent in valid_ranges:\n",
    "    accurate_preds = preds.copy()\n",
    "    size_matched_test = test.copy()\n",
    "    for col in traits:\n",
    "        accurate_preds = accurate_preds.loc[(accurate_preds[col] >= (size_matched_test[col]*(1-percent))) & (accurate_preds[col] <= (size_matched_test[col]*(1+percent)))]\n",
    "        size_matched_test = size_matched_test[size_matched_test['photo_id'].isin(accurate_preds['photo_id'])]\n",
    "    valid_ranges[percent] = accurate_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe8057",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print proportion of the predicted values that are within each margin of error of the test set.\n",
    "for percent in valid_ranges:\n",
    "    print(f'Accurate within ' + str(percent * 100) + \"% of test: \" + str(len(valid_ranges[percent]) / len(test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "2816c529d37cd6f072476ae02d8f4bbad83057dd1e1e56cd5cf923c572082e49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
